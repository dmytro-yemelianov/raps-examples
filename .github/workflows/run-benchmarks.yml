name: Run Benchmarks

on:
  push:
    branches: [master, main]
  pull_request:
  workflow_dispatch:

jobs:
  benchmarks:
    name: Run All Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install Python dependencies
        run: pip install matplotlib pandas jinja2 markdown

      - name: Install RAPS CLI
        run: |
          echo "Installing RAPS CLI..."
          # Download latest release
          RAPS_VERSION=$(curl -s https://api.github.com/repos/dmytro-yemelianov/raps/releases/latest | grep '"tag_name":' | sed -E 's/.*"([^"]+)".*/\1/')
          if [ -z "$RAPS_VERSION" ]; then
            RAPS_VERSION="v3.10.1"  # Fallback to known version
          fi
          echo "Installing RAPS $RAPS_VERSION"
          # Asset format: raps-cli-x86_64-unknown-linux-gnu.tar.xz
          ASSET_URL="https://github.com/dmytro-yemelianov/raps/releases/download/${RAPS_VERSION}/raps-cli-x86_64-unknown-linux-gnu.tar.xz"
          echo "Downloading from: $ASSET_URL"
          curl -fsSL "$ASSET_URL" -o raps.tar.xz || true
          if [ -f raps.tar.xz ]; then
            tar -xJf raps.tar.xz
            # Binary might be in a subdirectory
            find . -name "raps" -type f -executable | head -1 | xargs -I {} sudo mv {} /usr/local/bin/raps || true
            rm -f raps.tar.xz
            if command -v raps &> /dev/null; then
              echo "RAPS installed successfully"
              raps --version
            else
              echo "RAPS binary not found after extraction"
              ls -la
            fi
          else
            echo "RAPS release not found - benchmarks will run in mock mode"
          fi
        continue-on-error: true

      - name: Create directories
        run: mkdir -p reports data/generated

      - name: Make scripts executable
        run: |
          chmod +x scripts/*.sh scripts/*.py
          chmod +x benchmarks/*/*.sh

      - name: Generate test data
        run: |
          echo "Generating test data files..."
          python scripts/generate-test-data.py --output data/generated --size 100mb --name small-metadata
          python scripts/generate-test-data.py --output data/generated --size 500mb --name medium-metadata
          python scripts/generate-test-data.py --output data/generated --size 1gb --name large-metadata
          echo "Generated files:"
          ls -lh data/generated/*.json

      - name: Run Feature Validation
        run: ./benchmarks/feature-validation/run.sh
        env:
          REPORT_DIR: ./reports
        continue-on-error: true

      - name: Run Automation Timing
        run: ./benchmarks/automation-timing/run.sh
        env:
          REPORT_DIR: ./reports
        continue-on-error: true

      - name: Run Pipeline Timing
        run: ./benchmarks/pipeline-timing/run.sh
        env:
          REPORT_DIR: ./reports
        continue-on-error: true

      - name: Run Auth Flows
        run: ./benchmarks/auth-flows/run.sh
        env:
          REPORT_DIR: ./reports
          APS_CLIENT_ID: ${{ secrets.APS_CLIENT_ID }}
          APS_CLIENT_SECRET: ${{ secrets.APS_CLIENT_SECRET }}
        continue-on-error: true

      - name: Run Translation Performance
        run: ./benchmarks/translation-performance/run.sh
        env:
          REPORT_DIR: ./reports
        continue-on-error: true

      - name: Run Design Automation
        run: ./benchmarks/design-automation/run.sh
        env:
          REPORT_DIR: ./reports
        continue-on-error: true

      - name: Run Version Compatibility
        run: ./benchmarks/version-compatibility/run.sh
        env:
          REPORT_DIR: ./reports
        continue-on-error: true

      - name: Run Rust vs Node.js
        run: ./benchmarks/rust-vs-nodejs/run.sh
        env:
          REPORT_DIR: ./reports
          DATA_DIR: ./data/generated
        continue-on-error: true

      - name: Generate HTML Report
        run: python scripts/generate-report.py
        env:
          REPORT_DIR: ./reports

      - name: List reports
        run: ls -la reports/

      - name: Upload Benchmark Results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: reports/
          retention-days: 30

      - name: Add summary to job
        run: |
          echo "# Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f reports/metrics-report.md ]; then
            cat reports/metrics-report.md >> $GITHUB_STEP_SUMMARY
          else
            echo "Reports generated - see artifacts" >> $GITHUB_STEP_SUMMARY
          fi
